{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef5c701-8827-42fe-ab29-a0fb4b51315e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Below step is only for portfolio project. In real prod, need to use service principal to link storage account to Databricks\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "    \"fs.azure.account.key.stenergyplatformadls.dfs.core.windows.net\",\n",
    "    \"<access_key>"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8c61a1-3ca8-4ff9-8f32-131f95ccd466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://bronze@stenergyplatformadls.dfs.core.windows.net/energy_trends/prices/ET_5.1_SEP_25.xlsx', name='ET_5.1_SEP_25.xlsx', size=357763, modificationTime=1766741948000),\n",
       " FileInfo(path='abfss://bronze@stenergyplatformadls.dfs.core.windows.net/energy_trends/prices/ET_5.2_SEP_25.xlsx', name='ET_5.2_SEP_25.xlsx', size=118151, modificationTime=1766741939000)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validating access to storage account from Databricks\n",
    "dbutils.fs.ls(\"abfss://bronze@stenergyplatformadls.dfs.core.windows.net/energy_trends/prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29cfdcec-1b8f-43f2-a5e6-a95cc66429c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2b4db5-a668-4024-b3cd-26b78d4d3afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, coalesce, last, monotonically_increasing_id, lit, to_date, year, month,  regexp_extract, expr, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247e6f79-8723-4ee4-b3ff-c03209fec9d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/rajalakshmidharmaraj81@gmail.com/notebooks/common_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75eda98-568f-4ce5-a312-1487b4009bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbda57d3-64cc-43f7-ad99-3e09a18c83d7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766915782904}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def forward_fill_tablename(df_bronze):\n",
    "    #Adding a column to hold table name\n",
    "    df_with_table = df_bronze.withColumn(\n",
    "        \"table_name\",\n",
    "        when(col(\"generator_type\").rlike(\"(?i)table\"), col(\"generator_type\"))\n",
    "    )\n",
    "    #Repartitioning to 1 partition to have sequential row ids\n",
    "    df_with_table = df_with_table.repartition(1)\n",
    "\n",
    "    #Forward filling generator type and table names\n",
    "    df = df_with_table.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    df = df.withColumn('generator_type', coalesce(col('generator_type'), last('generator_type', True).over(Window.orderBy('row_id')), lit('0')))\n",
    "    df_filled = df.withColumn('table_name', coalesce(col('table_name'), last('table_name', True).over(Window.orderBy('row_id')), lit('0')))\n",
    "\n",
    "    df_data = df_filled.filter( ~col(\"generator_type\").rlike(\"(?i)table\"))\n",
    "    df_data = df_data.drop('row_id')\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56043d5-a6d8-46a8-9012-dcd519b8f668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_col_name(original_name, header_value):\n",
    "    \"\"\"\n",
    "    Applies all logic: header extraction, lowercase, character removal, \n",
    "    'fuel' mapping, and 'provisional' removal.\n",
    "    \"\"\"\n",
    "    # Use the value from the header row\n",
    "    name = str(header_value)\n",
    "    \n",
    "    # Apply your specific string replacements & lowercase\n",
    "    name = (name.lower()\n",
    "            .replace(\" \", \"_\")\n",
    "            .replace(\"(\", \"\")\n",
    "            .replace(\")\", \"\")\n",
    "            .replace(\"[\", \"\")\n",
    "            .replace(\"]\", \"\")\n",
    "            .replace(\"\\n\", \"_\"))\n",
    "    \n",
    "    # Logic: if 'fuel' is in the name, rename the whole thing to 'fuel'\n",
    "    if \"fuel\" in name:\n",
    "        return \"fuel\"\n",
    "    \n",
    "    # Logic: remove '_provisional'\n",
    "    name = name.replace(\"_provisional\", \"\")\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8ca34d-4a38-4e0e-a29d-e963fd02aedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def column_mapping(df_bronze):\n",
    "    # 1. First, extract the header values from the specific row\n",
    "    header_row = df_bronze.filter('_c0 like \"Generator type\"').limit(1).collect()[0]\n",
    "    current_cols = df_bronze.columns\n",
    "\n",
    "    # 2. Build the selection list using the zipped original column names and header row values\n",
    "    new_columns = [\n",
    "        col(old_name).alias(transform_col_name(old_name, header_row[i]))\n",
    "        for i, old_name in enumerate(current_cols)]\n",
    "    \n",
    "    # 3. Apply all changes in one single transformation\n",
    "    df_column_mapped = df_bronze.select(*new_columns)\n",
    "\n",
    "    return df_column_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2902ed-57e6-417f-a6f5-8cbed166aea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def round_float_columns(df):\n",
    "    # Identify columns that are Float or Double types\n",
    "    float_cols = [f.name for f in df.schema.fields \n",
    "                if isinstance(f.dataType, (FloatType, DoubleType))]\n",
    "\n",
    "    # Apply rounding to 2 decimal places for those specific columns\n",
    "    for column in float_cols:\n",
    "        df_cleaned = df.withColumn(column, round(col(column), 2))\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8401d4-b51e-481a-b0e2-8dd134e58baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_et51_tables(bronze_path, table):\n",
    "    \"\"\"\n",
    "    Reads the excel file from bronze_path and returns a cleaned dataframe\n",
    "    \"\"\"\n",
    "    if table == 'main':\n",
    "        excel_data_address = \"'Main Table'!A6\"\n",
    "    elif table == 'annual':\n",
    "        excel_data_address = \"'Annual'!A5\"\n",
    "    elif table == 'quarter':\n",
    "        excel_data_address = \"'Quarter'!A5\"\n",
    "    #Reading the excel file from bronze container\n",
    "    df_bronze_main = (\n",
    "        spark.read\n",
    "        .format(\"com.crealytics.spark.excel\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"dataAddress\", excel_data_address)  # sheet name\n",
    "        .load(bronze_path)\n",
    "        )\n",
    "    df_correct_columns = column_mapping(df_bronze_main)\n",
    "    df_with_tablename = forward_fill_tablename(df_correct_columns)\n",
    "\n",
    "    #Remove the header row from the data now that it's in the schema\n",
    "    df_cleaned = df_with_tablename.filter('generator_type not like \"Generator type\"')\n",
    "\n",
    "    #Rounding the float columns\n",
    "    # df_cleaned = round_float_columns(df_cleaned)\n",
    "    \n",
    "    #Cleaning up the values in 'fuel' column                    \n",
    "    df_final = df_cleaned.withColumn(\"fuel\", regexp_extract(col(\"fuel\"), r\"^([A-Za-z ]+)\", 1))\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10993e4a-ff22-487e-b9f6-16828f75098e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fe49bc-0dcf-440a-8b7f-34718ae35d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading ET5.1 file from bronze container\n",
    "bronze_path = \"abfss://bronze@stenergyplatformadls.dfs.core.windows.net/energy_trends/prices/ET_5.1_SEP_25.xlsx\"\n",
    "\n",
    "for table in ('main', 'annual', 'quarter'):\n",
    "  silver_path = f\"abfss://silver@stenergyplatformadls.dfs.core.windows.net/energy_trends_generation/{table}/\"\n",
    "  df = clean_et51_tables(bronze_path, table)\n",
    "  #Write cleaned data to appropriate silver path\n",
    "  df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f448c8-ed7b-49c0-96cf-f5ba4f6ff4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_to_silver_energy_prices.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}