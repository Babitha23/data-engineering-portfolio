{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below step is only for portfolio project. In real prod, need to use service principal to link storage account to Databricks\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "    \"fs.azure.account.key.stenergyplatformadls.dfs.core.windows.net\",\n",
    "    \"ACCESS_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, coalesce, last, monotonically_increasing_id, lit, to_date, year, month, regexp_extract, expr, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fill_tablename(df_bronze):\n",
    "    \"\"\"\n",
    "    Identifies table names within the generator_type column and forward-fills \n",
    "    them across subsequent rows to maintain context after filtering.\n",
    "    \n",
    "    Logic:\n",
    "    1. Extracts 'table_name' from rows where generator_type matches 'table'.\n",
    "    2. Uses a Window function over a monotonically increasing ID to forward-fill values.\n",
    "    3. Filters out the original header/metadata rows to keep only data rows.\n",
    "    \"\"\"\n",
    "    df_with_table = df_bronze.withColumn(\n",
    "        \"table_name\",\n",
    "        when(col(\"generator_type\").rlike(\"(?i)table\"), col(\"generator_type\"))\n",
    "    )\n",
    "    df_with_table = df_with_table.repartition(1)\n",
    "\n",
    "    df = df_with_table.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    df = df.withColumn('generator_type', coalesce(col('generator_type'), last('generator_type', True).over(Window.orderBy('row_id')), lit('0')))\n",
    "    df_filled = df.withColumn('table_name', coalesce(col('table_name'), last('table_name', True).over(Window.orderBy('row_id')), lit('0')))\n",
    "\n",
    "    df_data = df_filled.filter(~col(\"generator_type\").rlike(\"(?i)table\"))\n",
    "    df_data = df_data.drop('row_id')\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_col_name(original_name, header_value):\n",
    "    \"\"\"\n",
    "    Standardizes raw Excel header strings into clean, machine-readable column names.\n",
    "    \n",
    "    Logic:\n",
    "    - Converts to lowercase and replaces spaces/newlines with underscores.\n",
    "    - Removes special characters like parentheses and brackets.\n",
    "    - Normalizes 'fuel' related columns to a single 'fuel' identifier.\n",
    "    - Strips 'provisional' suffixes for schema consistency.\n",
    "    \"\"\"\n",
    "    name = str(header_value)\n",
    "    \n",
    "    name = (name.lower()\n",
    "            .replace(\" \", \"_\")\n",
    "            .replace(\"(\", \"\")\n",
    "            .replace(\")\", \"\")\n",
    "            .replace(\"[\", \"\")\n",
    "            .replace(\"]\", \"\")\n",
    "            .replace(\"\\n\", \"_\"))\n",
    "    \n",
    "    if \"fuel\" in name:\n",
    "        return \"fuel\"\n",
    "    \n",
    "    name = name.replace(\"_provisional\", \"\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_mapping(df_bronze):\n",
    "    \"\"\"\n",
    "    Orchestrates the renaming of a DataFrame based on a specific header row \n",
    "    found within the data.\n",
    "    \n",
    "    Logic:\n",
    "    1. Locates the row containing 'Generator type'.\n",
    "    2. Iterates through columns and applies the transform_col_name logic.\n",
    "    3. Returns a DataFrame with the new aliased schema.\n",
    "    \"\"\"\n",
    "    header_row = df_bronze.filter('_c0 like \"Generator type\"').limit(1).collect()[0]\n",
    "    current_cols = df_bronze.columns\n",
    "\n",
    "    new_columns = [\n",
    "        col(old_name).alias(transform_col_name(old_name, header_row[i]))\n",
    "        for i, old_name in enumerate(current_cols)]\n",
    "    \n",
    "    df_column_mapped = df_bronze.select(*new_columns)\n",
    "    return df_column_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_et51_tables(bronze_path, table):\n",
    "    \"\"\"\n",
    "    Reads the excel file from bronze_path and returns a cleaned dataframe\n",
    "    \"\"\"\n",
    "    if table == 'main':\n",
    "        excel_data_address = \"'Main Table'!A6\"\n",
    "    elif table == 'annual':\n",
    "        excel_data_address = \"'Annual'!A5\"\n",
    "    elif table == 'quarter':\n",
    "        excel_data_address = \"'Quarter'!A5\"\n",
    "    #Reading the excel file from bronze container\n",
    "    df_bronze = (\n",
    "        spark.read\n",
    "        .format(\"com.crealytics.spark.excel\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"dataAddress\", excel_data_address)  # sheet name\n",
    "        .load(bronze_path)\n",
    "        )\n",
    "    df_correct_columns = column_mapping(df_bronze)\n",
    "    df_with_tablename = forward_fill_tablename(df_correct_columns)\n",
    "\n",
    "    #Remove the header row from the data now that it's in the schema and remove null rows\n",
    "    df_filtered = df_with_tablename.filter('generator_type not like \"Generator type\"').filter(col('fuel').isNotNull())\n",
    "\n",
    "    #Logic for the 'unit' column\n",
    "    df_with_newcols = df_filtered.withColumn(\"unit\", when(col(\"table_name\").contains(\"Mtoe\"), lit(\"mtoe\"))\n",
    "        .when(col(\"table_name\").contains(\"TWh\"), lit(\"TWh\"))\n",
    "        .when(col(\"table_name\").contains(\"%\"), lit(\"percentage\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    df_with_newcols = df_with_newcols.withColumn(\"unit\", when(col(\"fuel\").contains(\"M tonnes\"), \"mtoe\")\n",
    "                                        .when(col(\"fuel\").contains(\"TWh\"), \"TWh\")\n",
    "                                        .otherwise(col(\"unit\")))\\\n",
    "                                    .withColumn(\"ingestion_date\", lit(datetime.now()))\n",
    "    \n",
    "    #Cleaning the 'fuel' column\n",
    "\n",
    "    # We chain the replacements: remove units, then remove any [note X] patterns\n",
    "    # \\s* handles optional spaces before the brackets\n",
    "    note_pattern = r\"\\s*\\[note \\d+\\]\"\n",
    "    unit_patterns = r\"\\s*\\(M tonnes\\)|\\s*\\(TWh\\)\"\n",
    "\n",
    "    df_clean = df_with_newcols.withColumn(\"fuel\", \n",
    "        regexp_replace(col(\"fuel\"), unit_patterns, \"\") # Remove (M tonnes) or (TWh)\n",
    "    ).withColumn(\"fuel\", \n",
    "        regexp_replace(col(\"fuel\"), note_pattern, \"\") # Remove [note X]\n",
    "    )\n",
    "\n",
    "    # To remove [note X] from EVERY string column in the dataframe\n",
    "    for col_name in [c for c, t in df.dtypes if t == 'string']:\n",
    "        df_clean = df_clean.withColumn(col_name, regexp_replace(col(col_name), note_pattern, \"\"))\n",
    "\n",
    "    #Casting datatypes for all the numeric columns\n",
    "    cols_to_fix = [c for c in df_clean.columns if c not in [\"generator_type\", \"fuel\", \"table_name\", \"unit\", \"ingestion_date\"]]\n",
    "\n",
    "    # Apply transformations: cast to float, fill nulls with 0, and round to 2 decimals\n",
    "    df_final = df_clean.select(\n",
    "        \"generator_type\", \"fuel\", \"table_name\", \"unit\", \"ingestion_date\",\n",
    "        *[round(coalesce(col(c).cast(\"double\"), lit(0)), 2).alias(c) for c in cols_to_fix]\n",
    "    )\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"abfss://bronze@stenergyplatformadls.dfs.core.windows.net/energy_trends/ET_5.1_SEP_25.xlsx\"\n",
    "\n",
    "for table in ('main', 'annual', 'quarter'):\n",
    "    silver_path = f\"abfss://silver@stenergyplatformadls.dfs.core.windows.net/energy_trends_generation/{table}/\"\n",
    "    df = clean_et51_tables(bronze_path, table)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dbsparkutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
